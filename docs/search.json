[
  {
    "objectID": "slides/01-introduction.html#hello-my-name-is-byron",
    "href": "slides/01-introduction.html#hello-my-name-is-byron",
    "title": "MELODEM data workshop",
    "section": "Hello! My name is Byron",
    "text": "Hello! My name is Byron\n\nI am an R enthusiast.\nI love dogs.\nI study risk  prediction + machine learning"
  },
  {
    "objectID": "slides/01-introduction.html#schedule-for-day-1-morning",
    "href": "slides/01-introduction.html#schedule-for-day-1-morning",
    "title": "MELODEM data workshop",
    "section": "Schedule for day 1: Morning",
    "text": "Schedule for day 1: Morning\nIntroduction, data management (75m)\n\nBreak (15m)\n\nDecision trees and random forests (75m)\n\nBreak (15m)\n\nLunch (1 hour)"
  },
  {
    "objectID": "slides/01-introduction.html#schedule-for-day-1-afternoon",
    "href": "slides/01-introduction.html#schedule-for-day-1-afternoon",
    "title": "MELODEM data workshop",
    "section": "Schedule for day 1: Afternoon",
    "text": "Schedule for day 1: Afternoon\nDevelop and evaluate prediction models (75m)\n\nBreak (15m)\n\nDevelop and evaluate causal models (75m)\n\nBreak (15m)\n\nCollaboration (1 hour)"
  },
  {
    "objectID": "slides/01-introduction.html#schedule-for-day-2-morning",
    "href": "slides/01-introduction.html#schedule-for-day-2-morning",
    "title": "MELODEM data workshop",
    "section": "Schedule for day 2: Morning",
    "text": "Schedule for day 2: Morning\nReview and discuss manuscript aims (30m)\n\nAlign on aims and responsibilities\n\nThree break-out sessions (60m each)\n\nWork independently or in small groups (45m)\nProgress updates (10m)\nBreak (5m)\n\nWrap-up (30m)"
  },
  {
    "objectID": "slides/01-introduction.html#set-up-r-packages",
    "href": "slides/01-introduction.html#set-up-r-packages",
    "title": "MELODEM data workshop",
    "section": "Set-up R packages",
    "text": "Set-up R packages\nMake sure we all have up-to-date versions of these R packages:\n\n# Install required packages for the workshop\npkgs &lt;- \n  c(\"tidyverse\", \"tidymodels\", \"data.table\", \"haven\", \"magrittr\",\n    \"glue\", \"grf\", \"aorsf\", \"glmnet\", \"xgboost\", \"randomForestSRC\",\n    \"party\", \"riskRegression\", \"survival\", \"officer\", \"flextable\", \n    \"table.glue\", \"gtsummary\", \"usethis\", \"cli\")\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "slides/01-introduction.html#set-up-github",
    "href": "slides/01-introduction.html#set-up-github",
    "title": "MELODEM data workshop",
    "section": "Set-up GitHub",
    "text": "Set-up GitHub\nMake sure we all have GitHub account with personal access token (PAT) stored in Rstudio\n\n\n\nOpen Rstudio\nCopy/paste the code on this slide into an R script\nImportant: adjust destdir\nRun\n\n\n\nlibrary(usethis)\n\ncreate_from_github(\n  \"bcjaeger/melodem-apoe4-het\",\n  destdir = \"path/of/choice\", \n  fork = TRUE\n)"
  },
  {
    "objectID": "slides/01-introduction.html#introducing-targets",
    "href": "slides/01-introduction.html#introducing-targets",
    "title": "MELODEM data workshop",
    "section": "Introducing targets",
    "text": "Introducing targets"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn",
    "href": "slides/01-introduction.html#your-turn",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\n\nOpen _targets.R in the melodem-apoe4-het project.\nRun library(targets) to load the targets package.\nRun tar_load_globals() to load relevant functions and packages.\nRun tar_glimpse() to inspect the pipeline.\nRun tar_make() to make the pipeline.\n\nWhile you’re working,\n\nPlace red sticky note on the back of your laptop if you want help.\nPlace green sticky note on the back of your laptop when you finish."
  },
  {
    "objectID": "slides/01-introduction.html#working-together-separately.",
    "href": "slides/01-introduction.html#working-together-separately.",
    "title": "MELODEM data workshop",
    "section": "Working together, separately.",
    "text": "Working together, separately.\nWhy do we need all the bells and whistles?\n\nManagement of multiple datasets (targets)\nCoordinating analyses over these datasets (targets)\nLarge amount of code from multiple authors (GitHub)\nCollaborative discussions in public makes better science (GitHub)"
  },
  {
    "objectID": "slides/01-introduction.html#start-with-data-management",
    "href": "slides/01-introduction.html#start-with-data-management",
    "title": "MELODEM data workshop",
    "section": "Start with data management",
    "text": "Start with data management\nIn the _targets.R file:\n\n\n\nfile_sim_1_tar &lt;- tar_target(\n  file_sim_1,\n  command = \"data/sim_1-raw.csv\",\n  format = 'file'\n)\n\ndata_sim_1_tar &lt;- tar_target(\n  data_sim_1,\n  data_prepare(file_sim_1, \n               age_range = c(55, 80))\n)\n\n\n\n\n\nThis will be done with your data, too!"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-1",
    "href": "slides/01-introduction.html#your-turn-1",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\nThink of a name for your data.\n\nExample name: regards\n\nSave a copy of your data in data/sensitive. The name of your file should be name-raw.csv or name-raw.sas7bdat, where name is your data’s name. E.g., regards-raw.csv"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-2",
    "href": "slides/01-introduction.html#your-turn-2",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\nSwitch from the R console to the terminal.\nVerify you have no uncommitted changes:\n\n\ngit status\n\nShould return “nothing to commit, working tree clean”\n\nCreate a new branch with git:\n\n\ngit branch -b regards"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-3",
    "href": "slides/01-introduction.html#your-turn-3",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\n\n\nModify the code beside in _targets.R, replacing x with the name of your data.\nSave the _targets.R file\nRun tar_make() in the R console.\nrun tar_load(data_x), where x is your data name, and print it\n\n\n\nfile_x_tar &lt;- tar_target(               \n  file_x,\n  command = \"data/sensitive/x-raw.csv\",\n  format = \"file\"\n)\n\ndata_x_tar &lt;- tar_target(\n  data_x,\n  data_prepare(file_name = \"data/sensitive/x-raw.csv\")\n)\n\n# don't forget to add these targets\n# to the targets list at the bottom!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "",
    "text": "These are the materials for the MEthods in LOngitudinal DEMentia (MELODEM) workshops offered at Château de Bellinglise, France, July 2024.\nThe workshop focuses on the use of random forests to predict dementia risk and identify heterogeneity in the association between APOE4 and dementia risk. This website hosts the materials for the workshop and the corresponding GitHub repository (todo: add link here) includes additional materials for manuscript development."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "",
    "text": "These are the materials for the MEthods in LOngitudinal DEMentia (MELODEM) workshops offered at Château de Bellinglise, France, July 2024.\nThe workshop focuses on the use of random forests to predict dementia risk and identify heterogeneity in the association between APOE4 and dementia risk. This website hosts the materials for the workshop and the corresponding GitHub repository (todo: add link here) includes additional materials for manuscript development."
  },
  {
    "objectID": "index.html#data-set-requirements",
    "href": "index.html#data-set-requirements",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Data set requirements",
    "text": "Data set requirements\nData sets must have the following characteristics:\n\na right-censored time-to-dementia outcome.\ndata on APOE4 (any vs none, at minimum).\ndata on age and sex.\nYou personally are able to access it from the venue in France (e.g., Wifi, hard drive)\n\nIf the dataset doesn’t meet all of these criteria, it will not be eligible for the workshop analysis.\nFor efficiency during the workshop, please ensure the names of the required variables are as follows:\n\nTime to dementia should be named ‘time’ and should be numeric time in years from baseline assessment to the time of censoring, death, or dementia, whichever occurred first.\nDementia status should be named ‘status’ and should take values of 0 or 1, with 1 occurring if and only if dementia occurred before censoring and death.\nApoe4 should be named ‘apoe4’ and should have values of ‘carrier’ vs. ‘non_carrier’. Keep this binary for the sake of using it in causal random forests.\nAge should be named ‘age’ and should be numeric with age in years.\nSex should be named ‘sex’ and should include values of ‘male’ or ‘female’. This does not need to be binary if your data have a large sample (i.e., &gt; 250 observations with &gt;10 dementia events) of people who do not identify in either of those two categories\n\nData sets should have as many predictors of dementia risk possible. Predictors would include any variable that\n\nMay be associated with dementia risk\nMay be associated with APOE4 status\nIs not an effect of dementia or cognitive impairment\nMay explain heterogeneity in the association of APOE4 with dementia risk.\n\nData sets need not have an exhaustive set of predictors, but any predictors must be measured before or at the start of follow-up for the right-censored dementia outcome. Because the analysis will be exploratory and focus on the identifying heterogeneity in the association of APOE4 with incident dementia, we encourage everyone to include as many valid predictors from their data as feasible. A few examples include (but are not limited to):\n\neducation,\nrace/ethnicity,\nsmoking history,\nmid-life htn,\nmid-life obesity,\nhtn drugs and statins,\nSES,\ndiabetes,\nrenal disease,\natrial fibrillation,\ncardiovascular disease,\ndepression,\nhead trauma,\ninsomnia,\northostatic hypotension,\nfamily history of cognitive impairment,\nfamily history of Alzheimer’s Disease"
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://posit.co/download/rstudio-desktop/\nA recent version of git, available at https://git-scm.com/downloads\nThe following R packages, which you can install from the R console:\n\n\n# Install required packages for the workshop\npkgs &lt;- \n  c(\"tidyverse\", \"tidymodels\", \"data.table\", \"haven\", \"magrittr\",\n    \"glue\", \"grf\", \"aorsf\", \"glmnet\", \"xgboost\", \"randomForestSRC\",\n    \"party\", \"riskRegression\", \"survival\", \"officer\", \"flextable\", \n    \"table.glue\", \"gtsummary\", \"usethis\", \"cli\", \"ggforce\",\n    \"rpart\", \"rpart.plot\", \"ranger\", \"withr\", \"gt\", 'recipes', \n    'butcher')\n\n# A few more packages may be added - this list isn't final yet.\n\ninstall.packages(pkgs)\n\nIf you’re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here.\n\nGitHub\nA major aim of the workshop will be to establish a baseline of collaboration for developing and publishing a manuscript. We will use GitHub to facilitate this.\n\nIf you do not have a GitHub account, please create one here: https://github.com/\nIf you have a GitHub account, make sure you have a personal access token for HTTPS stored in your local Rstudio. Instructions to do this are provided here: https://happygitwithr.com/https-pat\n\n\n\ntargets\nIn addition to using GitHub, we will also use targets to coordinate our workflow. There will be a brief tutorial for targets during the workshop, but getting familiar with this R package before the workshop is highly encouraged. A full textbook on targets is available: https://books.ropensci.org/targets/.\nReading the following sections prior to the workshop will be very helpful:\n\nIntroduction: https://books.ropensci.org/targets/#intro\nA walk through: https://books.ropensci.org/targets/walkthrough.html\nFunction-based workflow: https://books.ropensci.org/targets/functions.html"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participants’ convenience. They are not meant as standalone learning materials.\n\nDay 1: Oblique and causal random forests\n\nIntroduction\nDecision trees and random forests\nOblique random forests\nCausal random forests\n\n\n\nDay 2: Collaborative analysis and manuscript planning"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto and is based on the fantastic workshops developed by the tidymodels team. Please submit an issue (todo: add link to github repo) on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e. not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-4",
    "href": "slides/01-introduction.html#your-turn-4",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\ndata_sim_1\n\n\n\n------------------------------------- sim_1 ------------------------------------- \n# A tibble: 974 × 8\n   time status apoe4    sex      age biomarker_1 biomarker_2 biomarker_3\n  &lt;dbl&gt;  &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1  3.75      0 normal   female  66.3      0.747      -0.742       -1.71 \n2  4.29      1 elevated female  65.5     -0.562      -0.0513       0.546\n3  5.60      0 normal   male    71.4      0.0133     -0.184        0.826\n4  2.02      1 elevated female  59.5      0.795      -1.32        -1.28 \n5  5.73      0 normal   female  58.5     -0.293       0.191        2.54 \n# ℹ 969 more rows\n\n ----------------------------------  exclusions  ---------------------------------- \n# A tibble: 2 × 2\n  label              n_obs\n  &lt;glue&gt;             &lt;int&gt;\n1 sim_1 participants  1000\n2 Aged 55-80 years     974"
  },
  {
    "objectID": "slides/01-introduction.html#data-management",
    "href": "slides/01-introduction.html#data-management",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nWe used data_prepare() to make this object.\nLet’s check out what data_prepare does.\n\ndata_prepare &lt;- function(file_name, ...){\n\n  output &lt;- data_load(file_name) %&gt;%\n    data_clean() %&gt;%\n    data_derive() %&gt;%\n    data_select() %&gt;%\n    data_exclude(...)\n\n  check_names(output$values,\n              c(\"age\", \"sex\", \"apoe4\", \"time\", \"status\"))\n\n  output\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-1",
    "href": "slides/01-introduction.html#data-management-1",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nWe used data_prepare() to make this object.\nLet’s check out what data_prepare does. First, it loads the data\n\ndata_prepare &lt;- function(file_name, ...){\n\n  output &lt;- data_load(file_name) %&gt;%\n    data_clean() %&gt;%\n    data_derive() %&gt;%\n    data_select() %&gt;%\n    data_exclude(...)\n\n  check_names(output$values,\n              c(\"age\", \"sex\", \"apoe4\", \"time\", \"status\"))\n\n  output\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-2",
    "href": "slides/01-introduction.html#data-management-2",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nLet’s check out data_load().\n\ndata_load &lt;- function(file_path){\n\n    # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(cohort_name, 'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-3",
    "href": "slides/01-introduction.html#data-management-3",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nLet’s check out data_load(). The object returned from this function includes data and a preliminary exclusion table.\n\ndata_load &lt;- function(file_path){\n\n    # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(cohort_name, 'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#sowhy-does-this-matter",
    "href": "slides/01-introduction.html#sowhy-does-this-matter",
    "title": "MELODEM data workshop",
    "section": "So…why does this matter?",
    "text": "So…why does this matter?\nEach dataset is unique, and some may require customized preparation:\n\nDifferent elements need to be cleaned.\nDifferent variables need to be derived.\nDifferent variables may be selected.\nDifferent exclusions may be applied.\n\ndata_load makes its output have a customized class based on the name of the dataset so that you, the owner of the data, are in control of these steps that may be uniquely defined for your data."
  },
  {
    "objectID": "slides/01-introduction.html#whole-game",
    "href": "slides/01-introduction.html#whole-game",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nFirst, you pull code down from the GitHub repo."
  },
  {
    "objectID": "slides/01-introduction.html#whole-game-1",
    "href": "slides/01-introduction.html#whole-game-1",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nNext, you commit code and summary results. No data!"
  },
  {
    "objectID": "slides/01-introduction.html#whole-game-2",
    "href": "slides/01-introduction.html#whole-game-2",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nLast, you push your code and summary results to the GitHub repo."
  },
  {
    "objectID": "slides/01-introduction.html#why-github",
    "href": "slides/01-introduction.html#why-github",
    "title": "MELODEM data workshop",
    "section": "Why GitHub?",
    "text": "Why GitHub?\nSo we can work together, separately!\n\nStore and coordinate code from multiple authors\nPublic facing team science\nFree website for our work (i.e., this workshop)."
  },
  {
    "objectID": "slides/01-introduction.html#pull",
    "href": "slides/01-introduction.html#pull",
    "title": "MELODEM data workshop",
    "section": "Pull!",
    "text": "Pull!\nMake sure you have a GitHub account with personal access token (PAT) stored in Rstudio\n\n\n\nOpen Rstudio\nCopy/paste the code on this slide into an R script\nImportant: adjust destdir\nRun\n\n\n\nlibrary(usethis)\n\ncreate_from_github(\n  \"bcjaeger/melodem-apoe4-het\",\n  destdir = \"path/of/choice\", \n  fork = TRUE\n)"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-4",
    "href": "slides/01-introduction.html#data-management-4",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nThe object returned also has customized class based on the dataset. The output also belongs to a broader class called melodem_data\n\ndata_load &lt;- function(file_path){\n\n  # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(cohort_name, 'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#why",
    "href": "slides/01-introduction.html#why",
    "title": "MELODEM data workshop",
    "section": "Why?",
    "text": "Why?\nEach dataset is unique, and some may require customized preparation:\n\nDifferent elements need to be cleaned.\nDifferent variables need to be derived.\nDifferent variables may be selected.\nDifferent exclusions may be applied.\n\ndata_load makes its output have a customized class based on the name of the dataset so that you, the owner of the data, are in control of these steps that may be uniquely defined for your data."
  },
  {
    "objectID": "slides/01-introduction.html#how",
    "href": "slides/01-introduction.html#how",
    "title": "MELODEM data workshop",
    "section": "How?",
    "text": "How?\nR’s generic function system. Generic functions (e.g., plot()) dispatch different methods depending on the type of input object.\n\nHere’s a look at the generic function for cleaning an object of class sim_1:\n\ndata_clean.sim_1 &lt;- function(data){\n  dt &lt;- as.data.table(data$values)\n  dt[, age := age * 5 + 65]\n  dt[, sex := fifelse(sex &gt; 0, 1, 0)]\n  dt[, sex := factor(sex, levels = c(0, 1),\n                     labels = c(\"male\", \"female\"))]\n  data$values &lt;- dt\n  data\n}"
  },
  {
    "objectID": "slides/01-introduction.html#how-1",
    "href": "slides/01-introduction.html#how-1",
    "title": "MELODEM data workshop",
    "section": "How?",
    "text": "How?\nR’s generic function system. Generic functions (e.g., plot()) dispatch different methods depending on the type of input object.\n\nHere’s the generic function for cleaning an object of class melodem_data:\n\ndata_clean.melodem_data &lt;- function(data){\n\n  data\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#your-data",
    "href": "slides/01-introduction.html#your-data",
    "title": "MELODEM data workshop",
    "section": "Your data",
    "text": "Your data\n\nYour data’s first class is the name you picked, e.g., regards, and second class is melodem_data. Verify by running class()\nWhen you run data_clean() with, e.g., the regards data,\n\nR will first look for a function called data_clean.regards\nIf it doesn’t exist, R runs data_clean.melodem_data\n\n\nTLDR: If you don’t write a specific function for data_clean, data_derive, etc. for your data, then these functions will not do anything to your data."
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-5",
    "href": "slides/01-introduction.html#your-turn-5",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nFor the rest of this session,\n\nImplement specific data_clean, data_derive, data_select, and data_exclude for your data.\nIf you already did these operations before the workshop, move the code you used into the corresponding function.\nIf you finish early, help someone else!\n\nAt a minimum, make sure your data have time/status for dementia, apoe4, age, and sex, so that you will be able to use your data for the rest of the workshop.\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#planting-seeds-for-causal-trees",
    "href": "slides/02-trees_and_forests.html#planting-seeds-for-causal-trees",
    "title": "Decision trees and random forests",
    "section": "Planting seeds for causal trees",
    "text": "Planting seeds for causal trees\nSuppose we have outcome \\(Y\\) and treatment \\(W\\in\\{0, 1\\}\\).\n\\[\\text{Define }\\tau = E[Y|W=1] - E[Y|W=0],\\]\nIf \\(\\tau\\) can be estimated conditional on a person’s characteristics, then a decision tree could be grown using \\(\\hat\\tau_1 \\mid x_1, \\ldots, \\hat\\tau_n \\mid x_n\\) as an outcome. That tree could predict who benefits from treatment and explain why.\n\nBut how do we get \\(\\hat\\tau_i \\mid x_i\\)? More on this later…"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests",
    "href": "slides/02-trees_and_forests.html#random-forests",
    "title": "MELODEM data workshop",
    "section": "Random Forests",
    "text": "Random Forests\nIf we have to make a yes/no decision, who should make it?\n\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-1",
    "href": "slides/02-trees_and_forests.html#random-forests-1",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\nIf we have to make a yes/no decision, who should make it?\n\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-2",
    "href": "slides/02-trees_and_forests.html#random-forests-2",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\nIf we have to make a yes/no decision, who should make it?\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!\n\nWhy? Weak expert majority is right ~92% of the time\n\n# probability that &gt;2500 weak experts are right\n# = 1 - probability that &lt;=2500 are right\n1 - pbinom(q = 2500, size = 5000, prob = 0.51)\n\n[1] 0.9192858"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-3",
    "href": "slides/02-trees_and_forests.html#random-forests-3",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\nThe weak expert approach for decision trees:\n\nGrow each tree with a random subset of the training data.\n\n“In-bag” means in the random subset\n“Out-of-bag” means not in the subset\n\nEvaluate a random subset of predictors when splitting data.\n\nThese random elements help make the trees more independent while keeping their prediction accuracy better than random guesses."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#axis-based-and-oblique-trees",
    "href": "slides/02-trees_and_forests.html#axis-based-and-oblique-trees",
    "title": "Decision trees and random forests",
    "section": "Axis based and oblique trees",
    "text": "Axis based and oblique trees\nAxis based trees use a single predictor to split data.\nOblique trees use a weighted combination of two or more predictors"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#axis-based-vs-oblique",
    "href": "slides/02-trees_and_forests.html#axis-based-vs-oblique",
    "title": "Decision trees and random forests",
    "section": "Axis based vs oblique",
    "text": "Axis based vs oblique\n\nBreiman (2001) found oblique random forests compared more favorably to boosting than axis based ones.1\nMenze et al. (2011) coined the term ‘oblique’ random forest and introduced variable importance metrics for it.\nOn benchmarking 190 classifiers on 121 public datasets, Katuwal, Suganthan, and Zhang (2020) found variations on the oblique random forests were the top 3 classifiers.\n\nLeo Breiman named it “Forest-RC” but it later came to be known as oblique"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#aorsf-benchmark",
    "href": "slides/02-trees_and_forests.html#aorsf-benchmark",
    "title": "Decision trees and random forests",
    "section": "aorsf Benchmark",
    "text": "aorsf Benchmark\nJaeger et al. (2024) tested how a fast version of the oblique random survival forest (aorsf) compared to the original (obliqueRSF). Here’s what we did:\n\nEvaluated both types of oblique random forests in 35 risk prediction tasks (21 datasets)\nMeasured computation time, C-statistic and index of prediction accuracy (IPA).\nUsed Bayesian linear mixed models to test for differences in expected C-statistic and IPA."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nDecision trees are grown by recursively splitting a set of training data."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-1",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-1",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nFor classification, “Gini impurity” measures split quality:\n\\[G = 1 - \\sum_{i = 1}^{K} P(i)^2\\]\n\\(K\\) is no. of classes, \\(P(i)\\) is the probability of class \\(i\\)."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-2",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-2",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nConsider splitting at flipper length of 206.5: Right node:\n\ngini_right &lt;- penguins %&gt;% \n filter(flipper_length_mm&gt;=206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_right\n\n[1] 0.107008"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-3",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-3",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nConsider splitting at flipper length of 206.5: Left node:\n\ngini_left &lt;- penguins %&gt;% \n filter(flipper_length_mm&lt;206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_left\n\n[1] 0.4289479"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-4",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-4",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nConsider splitting at flipper length of 206.5: Impurity:\n\nsplit_var &lt;- penguins %&gt;% \n  pull(flipper_length_mm)\n\nn_tot &lt;- length(split_var)\nn_right &lt;- sum(split_var &gt;= 206.5)\nn_left &lt;- sum(split_var &lt; 206.5)\n\ngini_right * (n_right / n_tot) + \n  gini_left * (n_left / n_tot)\n\n[1] 0.3080996"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-5",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-5",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nNow we have two potential datasets, or nodes in the tree, that we can split.\nDo we keep going or stop?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-6",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-6",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nWe may stop tree growth if the node has:\n\nObs &lt; min_obs\nCases &lt; min_cases\nImpurity &lt; min_impurity\nDepth = max_depth"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-7",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-7",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nSuppose\n\nmin_obs = 10\nmin_cases = 2\nmin_impurity = 0.2\nmax_depth = 3\n\nWhich node(s) can we split?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-8",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-8",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\n\nThe left node can be split.\nThe right node cannot, since impurity on the right is &lt; min_impurity"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#partitions-as-a-tree",
    "href": "slides/02-trees_and_forests.html#partitions-as-a-tree",
    "title": "MELODEM data workshop",
    "section": "Partitions as a tree",
    "text": "Partitions as a tree\nThe same partitions, visualized as a binary tree."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-9",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-9",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\n\nThe left node can be split.\nAfter splitting the left node, all nodes have impurity &lt; min_impurity.\nIf no more nodes to grow, convert partitioned sets into leaf nodes"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn",
    "href": "slides/02-trees_and_forests.html#your-turn",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\nWas this the best possible split? Let’s find out.\n\nOpen classwork/02-trees_and_forests.qmd\nCalculate the total impurity of splitting along bill length of 45 mm.\nReminder: Red sticky note if you’d like help, green sticky note when you are finished.\nHint: the answer is 0.492340868530637"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-10",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-10",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nWhat is a leaf node?\n\nA leaf node is a terminal node in the tree.\nPredictions for new data are stored in leaf nodes.\n\n\n\n\nmutate(\n  penguins,\n  node = case_when(\n    flipper_length_mm &gt;= 206.5 ~ 'leaf_1',\n    bill_length_mm &gt;= 43.35 ~ 'leaf_2',\n    TRUE ~ 'leaf_3'\n  )\n) %&gt;% \n  group_by(node) %&gt;% count(species) %&gt;% \n  mutate(n = n / sum(n)) %&gt;% \n  pivot_wider(values_from = n, names_from = species, \n              values_fill = 0)\n\n# A tibble: 3 × 4\n# Groups:   node [3]\n  node   Adelie Chinstrap Gentoo\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 leaf_1 0.016     0.04   0.944 \n2 leaf_2 0.0635    0.921  0.0159\n3 leaf_3 0.966     0.0345 0"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn-1",
    "href": "slides/02-trees_and_forests.html#your-turn-1",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\n\nSuppose\n\nmin_obs = 10\nmin_cases = 2\nmin_impurity = 0.2\nmax_depth = 3\n\nWhich node(s) can we split?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees-11",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees-11",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\nThe same partitions, visualized as a binary tree.\n\nAs a reminder, here is our ‘hand-made’ leaf data\n\n\n\n\n\n\n  \n    \n      \n      Adelie\n      Chinstrap\n      Gentoo\n    \n  \n  \n    leaf_1\n0.02\n0.04\n0.94\n    leaf_2\n0.06\n0.92\n0.02\n    leaf_3\n0.97\n0.03\n0.00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html",
    "href": "slides/02-trees_and_forests.html",
    "title": "MELODEM data workshop",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE, \n                      message = FALSE,\n                      warning = FALSE,\n                      dpi = 300,\n                      cache = TRUE,\n                      fig.height = 6.5,\n                      fig.align = 'center')\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\npenguins &lt;- drop_na(penguins)\n\nwithr::with_dir(\n  new = here::here(),\n  targets::tar_load(penguin_figs)\n)"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#footnotes",
    "href": "slides/02-trees_and_forests.html#footnotes",
    "title": "MELODEM data workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBreiman L. Random forests. Machine learning. 2001 Oct;45:5-32.↩︎\nKatuwal R, Suganthan PN, Zhang L. Heterogeneous oblique random forest. Pattern Recognition. 2020 Mar 1;99:107078.↩︎\nMenze BH, Kelm BM, Splitthoff DN, Koethe U, Hamprecht FA. On oblique random forests. Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, Part II 22 2011 (pp. 453-469). Springer Berlin Heidelberg.↩︎\nJaeger BC, Welden S, Lenoir K, Speiser JL, Segar MW, Pandey A, Pajewski NM. Accelerated and interpretable oblique random survival forests. Journal of Computational and Graphical Statistics. 2023 Aug 3:1-6.↩︎"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag",
    "href": "slides/02-trees_and_forests.html#out-of-bag",
    "title": "MELODEM data workshop",
    "section": "Out-of-bag",
    "text": "Out-of-bag\nOut of bag predictions are helpful for estimation and inference"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-1",
    "href": "slides/02-trees_and_forests.html#out-of-bag-1",
    "title": "MELODEM data workshop",
    "section": "Out-of-bag",
    "text": "Out-of-bag"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-2",
    "href": "slides/02-trees_and_forests.html#out-of-bag-2",
    "title": "MELODEM data workshop",
    "section": "Out-of-bag",
    "text": "Out-of-bag"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-3",
    "href": "slides/02-trees_and_forests.html#out-of-bag-3",
    "title": "MELODEM data workshop",
    "section": "Out-of-bag",
    "text": "Out-of-bag"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-4",
    "href": "slides/02-trees_and_forests.html#out-of-bag-4",
    "title": "MELODEM data workshop",
    "section": "Out-of-bag",
    "text": "Out-of-bag"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions\nFinal out-of-bag predictions:\n\\[\\frac{\\text{out-of-bag predictions}}{\\text{out-of-bag denominator}}\\]\n\nWith conventional bootstrap sampling, each observation has about a 36.8% chance of being out-of-bag for each tree.\nOut-of-bag predictions give an unbiased view of the forest’s prediction on new data\nThis allows for variable importance estimates and (we’ll see this later) valid inference in causal forests."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions-1",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions-1",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions-2",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions-2",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions-3",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions-3",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions-4",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions-4",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions-5",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions-5",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions\nFinal out-of-bag predictions:\n\\[\\frac{\\text{out-of-bag predictions}}{\\text{out-of-bag denominator}}\\]\n\nWith conventional bootstrap sampling, each observation has about a 36.8% chance of being out-of-bag for each tree.\nOut-of-bag predictions give an unbiased view of the forest’s prediction on new data\nThis allows for variable importance estimates and (we’ll see this later) valid inference in causal forests."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance",
    "href": "slides/02-trees_and_forests.html#variable-importance",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nFirst compute out-of-bag prediction accuracy.\nIn this case, classification accuracy is 96%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-1",
    "href": "slides/02-trees_and_forests.html#variable-importance-1",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nNext, permute the values of a given variable\nSee how one value of flipper length is permuted in the figure?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-2",
    "href": "slides/02-trees_and_forests.html#variable-importance-2",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nNext, permute the values of a given variable\nNow they are all permuted, and out-of-bag classification accuracy is now 61%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-3",
    "href": "slides/02-trees_and_forests.html#variable-importance-3",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nRinse and repeat for all variables.\nFor bill length, out-of-bag classification accuracy is reduced to 63%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-4",
    "href": "slides/02-trees_and_forests.html#variable-importance-4",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\nOnce all variables have been through this process:\n\\[\\text{Variable importance} = \\text{initial accuracy} - \\text{permuted accuracy}\\]\n\nFlipper length: 0.96 - 0.61 = 0.35\nBill length: 0.96 - 0.63 = 0.33\n\n\\(\\Rightarrow\\) flippers more important than bills for species prediction."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-4",
    "href": "slides/02-trees_and_forests.html#random-forests-4",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\nTo get a prediction from the random forest,\n\npredict the outcome with each tree\ntake the majority vote\n\nIn situations where trees predictions are continuous and you can’t take a majority vote, e.g., predicted 10-year risk for dementia, just take the mean of the tree’s predictions."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-5",
    "href": "slides/02-trees_and_forests.html#random-forests-5",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-6",
    "href": "slides/02-trees_and_forests.html#random-forests-6",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forests-7",
    "href": "slides/02-trees_and_forests.html#random-forests-7",
    "title": "Decision trees and random forests",
    "section": "Random Forests",
    "text": "Random Forests\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#overview",
    "href": "slides/02-trees_and_forests.html#overview",
    "title": "Decision trees and random forests",
    "section": "Overview",
    "text": "Overview\n\nDecision trees\n\nGrowing trees\nLeaf nodes\n\nRandom Forests\n\nOut-of-bag predictions\nVariable importance\n\nAxis-based and oblique\n\nBenchmarks\nSoftware"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#gini-impurity",
    "href": "slides/02-trees_and_forests.html#gini-impurity",
    "title": "Decision trees and random forests",
    "section": "Gini impurity",
    "text": "Gini impurity\n\nFor classification, “Gini impurity” measures split quality:\n\\[G = 1 - \\sum_{i = 1}^{K} P(i)^2\\]\n\\(K\\) is no. of classes, \\(P(i)\\) is the probability of class \\(i\\)."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-right-node",
    "href": "slides/02-trees_and_forests.html#first-split-right-node",
    "title": "Decision trees and random forests",
    "section": "First split: right node",
    "text": "First split: right node\n\nConsider splitting at flipper length of 206.5: Right node:\n\ngini_right &lt;- penguins %&gt;% \n filter(flipper_length_mm&gt;=206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_right\n\n[1] 0.107008"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-left-node",
    "href": "slides/02-trees_and_forests.html#first-split-left-node",
    "title": "Decision trees and random forests",
    "section": "First split: left node",
    "text": "First split: left node\n\nConsider splitting at flipper length of 206.5: Left node:\n\ngini_left &lt;- penguins %&gt;% \n filter(flipper_length_mm&lt;206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_left\n\n[1] 0.4289479"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-total-impurity",
    "href": "slides/02-trees_and_forests.html#first-split-total-impurity",
    "title": "Decision trees and random forests",
    "section": "First split: total impurity",
    "text": "First split: total impurity\n\nConsider splitting at flipper length of 206.5: Impurity:\n\nsplit_var &lt;- penguins %&gt;% \n  pull(flipper_length_mm)\n\nn_tot &lt;- length(split_var)\nn_right &lt;- sum(split_var &gt;= 206.5)\nn_left &lt;- sum(split_var &lt; 206.5)\n\ngini_right * (n_right / n_tot) + \n  gini_left * (n_left / n_tot)\n\n[1] 0.3080996"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#keep-growing",
    "href": "slides/02-trees_and_forests.html#keep-growing",
    "title": "Decision trees and random forests",
    "section": "Keep growing?",
    "text": "Keep growing?\n\nNow we have two potential datasets, or nodes in the tree, that we can split.\nDo we keep going or stop?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#stopping-conditions",
    "href": "slides/02-trees_and_forests.html#stopping-conditions",
    "title": "Decision trees and random forests",
    "section": "Stopping conditions",
    "text": "Stopping conditions\n\nWe may stop tree growth if the node has:\n\nObs &lt; min_obs\nCases &lt; min_cases\nImpurity &lt; min_impurity\nDepth = max_depth"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#split-the-left-node",
    "href": "slides/02-trees_and_forests.html#split-the-left-node",
    "title": "Decision trees and random forests",
    "section": "Split the left node",
    "text": "Split the left node\n\n\nThe left node can be split.\nThe right node cannot, since impurity on the right is &lt; min_impurity"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#finished-growing",
    "href": "slides/02-trees_and_forests.html#finished-growing",
    "title": "Decision trees and random forests",
    "section": "Finished growing",
    "text": "Finished growing\n\n\nThe left node can be split.\nAfter splitting the left node, all nodes have impurity &lt; min_impurity.\nIf no more nodes to grow, convert partitioned sets into leaf nodes"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#leaves",
    "href": "slides/02-trees_and_forests.html#leaves",
    "title": "Decision trees and random forests",
    "section": "Leaves?",
    "text": "Leaves?\n\nWhat is a leaf node?\n\nA leaf node is a terminal node in the tree.\nPredictions for new data are stored in leaf nodes.\n\n\n\n\nmutate(\n  penguins,\n  node = case_when(\n    flipper_length_mm &gt;= 206.5 ~ 'leaf_1',\n    bill_length_mm &gt;= 43.35 ~ 'leaf_2',\n    TRUE ~ 'leaf_3'\n  )\n) %&gt;% \n  group_by(node) %&gt;% count(species) %&gt;% \n  mutate(n = n / sum(n)) %&gt;% \n  pivot_wider(values_from = n, names_from = species, \n              values_fill = 0)\n\n# A tibble: 3 × 4\n# Groups:   node [3]\n  node   Adelie Chinstrap Gentoo\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 leaf_1 0.016     0.04   0.944 \n2 leaf_2 0.0635    0.921  0.0159\n3 leaf_3 0.966     0.0345 0"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn-2",
    "href": "slides/02-trees_and_forests.html#your-turn-2",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\nFit your own decision tree to the penguins data.\n\nOpen classwork/02-trees_and_forests.qmd\nComplete Exercise 2"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#from-partition-to-flowchart",
    "href": "slides/02-trees_and_forests.html#from-partition-to-flowchart",
    "title": "Decision trees and random forests",
    "section": "From partition to flowchart",
    "text": "From partition to flowchart\nThe same partitions, visualized as a binary tree.\n\nAs a reminder, here is our ‘hand-made’ leaf data\n\n\n\n\n\n\n  \n    \n      \n      Adelie\n      Chinstrap\n      Gentoo\n    \n  \n  \n    leaf_1\n0.02\n0.04\n0.94\n    leaf_2\n0.06\n0.92\n0.02\n    leaf_3\n0.97\n0.03\n0.00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#expert-of-committee",
    "href": "slides/02-trees_and_forests.html#expert-of-committee",
    "title": "Decision trees and random forests",
    "section": "Expert of committee?",
    "text": "Expert of committee?\nIf we have to make a yes/no decision, who should make it?\n\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#why-committee",
    "href": "slides/02-trees_and_forests.html#why-committee",
    "title": "Decision trees and random forests",
    "section": "Why committee?",
    "text": "Why committee?\nIf we have to make a yes/no decision, who should make it?\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!\n\nWhy? Weak expert majority is right ~92% of the time\n\n# probability that &gt;2500 weak experts are right\n# = 1 - probability that &lt;=2500 are right\n1 - pbinom(q = 2500, size = 5000, prob = 0.51)\n\n[1] 0.9192858"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forest-recipe",
    "href": "slides/02-trees_and_forests.html#random-forest-recipe",
    "title": "Decision trees and random forests",
    "section": "Random forest recipe",
    "text": "Random forest recipe\nThe weak expert approach for decision trees:\n\nGrow each tree with a random subset of the training data.\n\n“In-bag” means in the random subset\n“Out-of-bag” means not in the subset\n\nEvaluate a random subset of predictors when splitting data.\n\nThese random elements help make the trees more independent while keeping their prediction accuracy better than random guesses."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forest-predictions",
    "href": "slides/02-trees_and_forests.html#random-forest-predictions",
    "title": "Decision trees and random forests",
    "section": "Random forest predictions",
    "text": "Random forest predictions\nTo get a prediction from the random forest,\n\npredict the outcome with each tree\ntake the majority vote\n\nIn situations where trees predictions are continuous and you can’t take a majority vote, e.g., predicted 10-year risk for dementia, just take the mean of the tree’s predictions."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-tree-is-okay",
    "href": "slides/02-trees_and_forests.html#a-single-tree-is-okay",
    "title": "Decision trees and random forests",
    "section": "A single tree is okay",
    "text": "A single tree is okay\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#trees-is-better",
    "href": "slides/02-trees_and_forests.html#trees-is-better",
    "title": "Decision trees and random forests",
    "section": "100 trees is better",
    "text": "100 trees is better\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#more-trees-does-not-mean-overfitting",
    "href": "slides/02-trees_and_forests.html#more-trees-does-not-mean-overfitting",
    "title": "Decision trees and random forests",
    "section": "More trees does not mean overfitting",
    "text": "More trees does not mean overfitting"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#how-to-get-out-of-bag-predictions",
    "href": "slides/02-trees_and_forests.html#how-to-get-out-of-bag-predictions",
    "title": "Decision trees and random forests",
    "section": "How to get out-of-bag predictions",
    "text": "How to get out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#grow-tree-1",
    "href": "slides/02-trees_and_forests.html#grow-tree-1",
    "title": "Decision trees and random forests",
    "section": "Grow tree #1",
    "text": "Grow tree #1"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#store-its-predictions-and-denominators",
    "href": "slides/02-trees_and_forests.html#store-its-predictions-and-denominators",
    "title": "Decision trees and random forests",
    "section": "Store its predictions and denominators",
    "text": "Store its predictions and denominators"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#now-do-tree-2",
    "href": "slides/02-trees_and_forests.html#now-do-tree-2",
    "title": "Decision trees and random forests",
    "section": "Now do tree #2",
    "text": "Now do tree #2"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#accumulate-predictions-and-denominator",
    "href": "slides/02-trees_and_forests.html#accumulate-predictions-and-denominator",
    "title": "Decision trees and random forests",
    "section": "Accumulate predictions and denominator",
    "text": "Accumulate predictions and denominator"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#oblique-tree-first-split",
    "href": "slides/02-trees_and_forests.html#oblique-tree-first-split",
    "title": "Decision trees and random forests",
    "section": "Oblique tree: first split",
    "text": "Oblique tree: first split\n\nFirst, split mostly by bill length"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#oblique-tree-second-split",
    "href": "slides/02-trees_and_forests.html#oblique-tree-second-split",
    "title": "Decision trees and random forests",
    "section": "Oblique tree: second split",
    "text": "Oblique tree: second split\n\nFirst, split mostly by bill length\n\nSecond, make a triangle for the gentoo."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#oblique-random-forests",
    "href": "slides/02-trees_and_forests.html#oblique-random-forests",
    "title": "Decision trees and random forests",
    "section": "Oblique random forests",
    "text": "Oblique random forests\n\nAggregating randomized trees gives the oblique random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#surprisingly-different",
    "href": "slides/02-trees_and_forests.html#surprisingly-different",
    "title": "Decision trees and random forests",
    "section": "Surprisingly different!",
    "text": "Surprisingly different!\n\nDespite very many similarities, axis-based and oblique random forests may give different results."
  },
  {
    "objectID": "slides/03-prediction_forests.html#overview",
    "href": "slides/03-prediction_forests.html#overview",
    "title": "Oblique random forests with aorsf",
    "section": "Overview",
    "text": "Overview\n\nAxis-based and oblique\n\nBackground\nBenchmarks\nSoftware"
  },
  {
    "objectID": "slides/04-causal_forests.html#overview",
    "href": "slides/04-causal_forests.html#overview",
    "title": "Causal random forests with grf",
    "section": "Overview",
    "text": "Overview\nThis lecture focuses on the the causal random forest algorithm\n\nRobinson’s residual-on-residual regression\nCausal trees\nCausal random forest\nCausal random survival forest\nInference with CATE summaries"
  },
  {
    "objectID": "slides/04-causal_forests.html#background",
    "href": "slides/04-causal_forests.html#background",
    "title": "Causal random forests",
    "section": "Background",
    "text": "Background\nConsider the partially linear model:\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\] Assume:\n\n\\(E[\\varepsilon_i | X_i, W_i] = 0\\)\nuntreated outcome is given by unknown function \\(f\\),\na treatment assignment shifts the outcome by \\(\\tau\\)."
  },
  {
    "objectID": "slides/04-causal_forests.html#background-1",
    "href": "slides/04-causal_forests.html#background-1",
    "title": "Causal random forests",
    "section": "Background",
    "text": "Background\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\]\nwhere \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#background-2",
    "href": "slides/04-causal_forests.html#background-2",
    "title": "Causal random forests",
    "section": "Background",
    "text": "Background\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\] where \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#background-3",
    "href": "slides/04-causal_forests.html#background-3",
    "title": "Causal random forests",
    "section": "Background",
    "text": "Background\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\] where \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#background-4",
    "href": "slides/04-causal_forests.html#background-4",
    "title": "Causal random forests",
    "section": "Background",
    "text": "Background\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\] where \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#simulated-data",
    "href": "slides/04-causal_forests.html#simulated-data",
    "title": "Causal random forests",
    "section": "Simulated data",
    "text": "Simulated data\n\nFirst, load global objects and data:\n\ntargets::tar_load_globals()\ntar_load(data_sim_1)\n\nSecond, coerce data to grf format:\n\n# helper function for grf data prep \ndata_grf &lt;- data_coerce_grf(data_sim_1)\n\n# just a view of the X matrix\ndata_grf$X[1:5, 1:4]\n\n          age biomarker_1 biomarker_2 biomarker_3\n[1,] 66.27077  0.74731375 -0.74236839  -1.7127343\n[2,] 65.51047 -0.56219488 -0.05134715   0.5464561\n[3,] 71.41878  0.01334597 -0.18433741   0.8256376\n[4,] 59.47485  0.79510234 -1.32408422  -1.2779016\n[5,] 58.47673 -0.29311302  0.19057088   2.5377970\n\n\n\n\nThird, fit the causal forest:\n\nfit_grf &lt;- \n  causal_survival_forest(\n    X = data_grf$X, # covariates\n    Y = data_grf$Y, # time to event\n    W = data_grf$W, # treatment status\n    D = data_grf$D, # event status\n    horizon = 3, # 3-year horizon\n    target = 'RMST' # survival time\n  )\n\nfit_grf\n\nGRF forest object of type causal_survival_forest \nNumber of trees: 2000 \nNumber of training samples: 974 \nVariable importance: \n    1     2     3     4     5 \n0.234 0.342 0.206 0.194 0.023"
  },
  {
    "objectID": "slides/04-causal_forests.html#background-5",
    "href": "slides/04-causal_forests.html#background-5",
    "title": "Develop and evaluate prediction models",
    "section": "Background",
    "text": "Background\nMore formally:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), w = 1\\{X_i \\in \\mathcal{N}(x) \\}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html",
    "href": "slides/04-causal_forests.html",
    "title": "Develop and evaluate prediction models",
    "section": "",
    "text": "Causal random forest algorithm\n\nBackground\nThe role of prediction\nAdaptive neighborhoods\nConditional and ranked average treatment effects\n\nCausal random forest applications\n\nSimulated data\nYour cohort data"
  },
  {
    "objectID": "slides/01-introduction.html",
    "href": "slides/01-introduction.html",
    "title": "MELODEM data workshop",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE, \n                      message = FALSE,\n                      warning = FALSE,\n                      dpi = 300,\n                      cache = TRUE,\n                      fig.height = 6,\n                      fig.align = 'center')"
  },
  {
    "objectID": "slides/04-causal_forests.html#references",
    "href": "slides/04-causal_forests.html#references",
    "title": "Causal random forests with grf",
    "section": "References",
    "text": "References\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nCui, Yifan, Michael R Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu. 2023. “Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests.” Journal of the Royal Statistical Society Series B: Statistical Methodology 85 (2): 179–211.\n\n\nRobinson, Peter M. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica: Journal of the Econometric Society, 931–54.\n\n\nSemenova, Vira, and Victor Chernozhukov. 2021. “Debiased Machine Learning of Conditional Average Treatment Effects and Other Causal Functions.” The Econometrics Journal 24 (2): 264–89.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42.\n\n\nYadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. 2021. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv Preprint arXiv:2111.07966."
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-as-an-adaptive-neighborhood-finder",
    "href": "slides/04-causal_forests.html#random-forest-as-an-adaptive-neighborhood-finder",
    "title": "Develop and evaluate prediction models",
    "section": "Random forest as an adaptive neighborhood finder",
    "text": "Random forest as an adaptive neighborhood finder"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forest-neighborhoods",
    "title": "Causal random forests",
    "section": "Random forest neighborhoods",
    "text": "Random forest neighborhoods\nPull \\(Y_i\\) out of the loop that depends on \\(b\\):\n\\[\\begin{equation*}\n\\begin{split}\np &= \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|} \\\\\n&= \\sum_{i=1}^{n} Y_i \\sum_{b=1}^{B} \\frac{1\\{Xi \\in L_b(x)\\}} {B \\cdot |L_b(x)|} \\\\\n& = \\sum_{i=1}^{n} Y_i \\color{blue}{\\alpha_i(x)},\n\\end{split}\n\\end{equation*}\\]\n\n\\(\\alpha_i(x) \\propto\\) no. of times observation \\(i\\) lands in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-okay",
    "href": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-okay",
    "title": "Decision trees and random forests",
    "section": "A single traditional tree is okay",
    "text": "A single traditional tree is okay\n\nAggregating randomized trees gives the classic random forest"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-randomized-tree-struggles",
    "href": "slides/02-trees_and_forests.html#a-single-randomized-tree-struggles",
    "title": "Decision trees and random forests",
    "section": "A single randomized tree struggles",
    "text": "A single randomized tree struggles"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#five-randomized-trees-do-okay",
    "href": "slides/02-trees_and_forests.html#five-randomized-trees-do-okay",
    "title": "Decision trees and random forests",
    "section": "Five randomized trees do okay",
    "text": "Five randomized trees do okay"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-fine",
    "href": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-fine",
    "title": "Decision trees and random forests",
    "section": "A single traditional tree is fine",
    "text": "A single traditional tree is fine"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#trees-do-great",
    "href": "slides/02-trees_and_forests.html#trees-do-great",
    "title": "Decision trees and random forests",
    "section": "100 trees do great",
    "text": "100 trees do great"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#randomized-trees-do-great",
    "href": "slides/02-trees_and_forests.html#randomized-trees-do-great",
    "title": "Decision trees and random forests",
    "section": "100 randomized trees do great",
    "text": "100 randomized trees do great"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#randomized-trees---no-overfitting",
    "href": "slides/02-trees_and_forests.html#randomized-trees---no-overfitting",
    "title": "Decision trees and random forests",
    "section": "500 randomized trees - no overfitting!",
    "text": "500 randomized trees - no overfitting!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#references",
    "href": "slides/02-trees_and_forests.html#references",
    "title": "Decision trees and random forests",
    "section": "References",
    "text": "References\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-neighborhoods-1",
    "href": "slides/04-causal_forests.html#random-forest-neighborhoods-1",
    "title": "Causal random forests",
    "section": "Random forest neighborhoods",
    "text": "Random forest neighborhoods\nInstead of defining neighborhood boundaries, weight by similarity:\n\\[\n\\hat\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\] This “forest”-localized version of Robinson’s regression gives asymptotic guarantees for estimation and inference (Wager and Athey 2018):\n\nPointwise consistency for the true treatment effect.\nAsymptotically Gaussian and centered sampling distribution."
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-neighborhoods-2",
    "href": "slides/04-causal_forests.html#random-forest-neighborhoods-2",
    "title": "Causal random forests",
    "section": "Random forest neighborhoods",
    "text": "Random forest neighborhoods\nInstead of defining neighborhood boundaries, weight by similarity:\n\\[\n\\tau(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\] This “forest”-localized version of Robinson’s regression gives asymptotic guarantees for estimation and inference (Wager and Athey 2018):\n\nPointwise consistency for the true treatment effect.\nAsymptotically Gaussian and centered sampling distribution."
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-estimate-tau",
    "href": "slides/04-causal_forests.html#how-to-estimate-tau",
    "title": "Causal random forests with grf",
    "section": "How to estimate \\(\\tau\\)?",
    "text": "How to estimate \\(\\tau\\)?\nSuppose\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\]\nHow do we estimate \\(\\tau\\) when we do not know \\(f(X_i)\\)?\nDefine:\n\\[\\begin{align*}\n\ne(x) &= E[W_i | X_i=x] \\,\\, \\text{(Propensity score)} \\\\\n\nm(x) &= E[Y_i | X_i = x] = f(x) + \\tau e(x) \\,\\,\\,\\,\\, \\text{(Cndl. mean of } Y\\text{)}\n\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#use-propensity-and-conditional-mean",
    "href": "slides/04-causal_forests.html#use-propensity-and-conditional-mean",
    "title": "Causal random forests with grf",
    "section": "Use propensity and conditional mean",
    "text": "Use propensity and conditional mean\nRe-express the partial linear model in terms of \\(e(x)\\) and \\(m(x)\\):\n\\[\\begin{align*}\n\nY_i &= \\tau W_i + f(X_i) + \\varepsilon_i, \\,  \\\\\n\nY_i - \\tau e (x) &= \\tau W_i + f(X_i) - \\tau e(x) + \\varepsilon_i, \\, \\\\\n\nY_i - f(X_i) - \\tau e (x) &= \\tau W_i - \\tau e (x) + \\varepsilon_i, \\, \\\\\n\nY_i - m(x) &= \\tau (W_i - e(x)) + \\varepsilon_i, \\, \\\\\n\n\\end{align*}\\]\n\\(\\tau\\) can be estimated with residual-on-residual regression (Robinson 1988).\nHow? Plug in flexible estimates of \\(m(x)\\) and \\(e(x)\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#re-write-as-a-linear-model",
    "href": "slides/04-causal_forests.html#re-write-as-a-linear-model",
    "title": "Causal random forests with grf",
    "section": "Re-write as a linear model",
    "text": "Re-write as a linear model\nMore formally,\n\\[\n\\hat{\\tau} := \\text{lm}\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i)\\Biggr).\n\\]\n\nSuperscript \\(^i\\) denotes cross-fit estimates (Chernozhukov et al. 2018).\nCross-fitting: estimate something, e.g., \\(e(x)\\), using cross-validation.\nWhy? removes bias from over-fitting."
  },
  {
    "objectID": "slides/04-causal_forests.html#example",
    "href": "slides/04-causal_forests.html#example",
    "title": "Causal random forests with grf",
    "section": "Example",
    "text": "Example\n\nSuppose \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i\\)\n\n\\(\\tau\\) is 1/2\n\\(W_i\\) is randomized treatment\n\\(X_i\\) is a continuous covariate\n\\(f(X_i) = |X_i|\\)\n\nBy defn, \\(E[W_i] = 1/2\\) (why?).\n\n\n\n# Set up for const tau example\n\nset.seed(1)\ntau_truth &lt;- 1/2\nn &lt;- 1000\n\n# randomized treatment\nW &lt;- rbinom(n = n, size = 1, prob = 1/2)\n\n# continuous covariate\nX &lt;- rnorm(n = n)\n\n# outcome \nY &lt;- tau_truth * W + abs(X) + rnorm(n)\n\ndata &lt;- data.frame(Y=Y, X=X, W=W)"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-contd",
    "href": "slides/04-causal_forests.html#example-contd",
    "title": "Causal random forests",
    "section": "Example contd",
    "text": "Example contd\n\nFirst we’ll do it the wrong way.\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nCompute residuals and run Robinson’s regression.\nWhat’d we do wrong?\n\n\n\n\nlibrary(aorsf)\nlibrary(glue)\n\nfit_cmean &lt;- orsf(Y ~ X, data=data)\n\nm_x &lt;- predict(fit_cmean, new_data = data)\n\nresid_y &lt;- Y - m_x\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.396912939347596"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-contd-1",
    "href": "slides/04-causal_forests.html#example-contd-1",
    "title": "Causal random forests",
    "section": "Example contd",
    "text": "Example contd\n\nWe forgot about cross-fitting!\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nUse out-of-bag predictions.\nCompute residuals and run Robinson’s regression.\n\n\n\n\nm_x_oobag &lt;- predict(fit_cmean, oobag = TRUE)\n\nresid_y &lt;- Y - m_x_oobag\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.501440224576038"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-wrong",
    "href": "slides/04-causal_forests.html#example-done-wrong",
    "title": "Causal random forests with grf",
    "section": "Example done wrong",
    "text": "Example done wrong\n\nFirst we’ll do it the wrong way.\n\nFit a classical model to estimate conditional mean of \\(Y\\).\nCompute residuals and run Robinson’s regression.\nWhat’d we do wrong?\n\n\n\n\nlibrary(glue)\nlibrary(ggplot2)\n\nfit_cmean &lt;- lm(Y ~ X, data = data)\n\nm_x &lt;- predict(fit_cmean, new_data = data)\n\nresid_y &lt;- Y - m_x\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.462441735905266"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-right",
    "href": "slides/04-causal_forests.html#example-done-right",
    "title": "Causal random forests with grf",
    "section": "Example done right",
    "text": "Example done right\n\nWe forgot about cross-fitting!\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nUse out-of-bag predictions.\nCompute residuals and run Robinson’s regression.\n\n\n\n\nm_x_oobag &lt;- predict(fit_cmean, oobag = TRUE)\n\nresid_y &lt;- Y - m_x_oobag\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.495509155402128"
  },
  {
    "objectID": "slides/04-causal_forests.html#causal-trees",
    "href": "slides/04-causal_forests.html#causal-trees",
    "title": "Causal random forests",
    "section": "Causal trees",
    "text": "Causal trees\nCausal trees are much like standard decision trees, but they maximize\n\\[n_L \\cdot n_R \\cdot (\\hat{\\tau}_L-\\hat{\\tau}_R)^2\\]\nwhere residual-on-residual regression is used to estimate \\(\\hat{\\tau}_L\\) and \\(\\hat{\\tau}_R\\)\n\ngrf estimates \\(\\hat \\tau\\) once in the parent node and uses “influence functions” to approximate how \\(\\hat\\tau\\) would change if an observation moved from one child node to the other (Wager and Athey 2018).\nPredictions from leaves are \\(E[Y|W=1] - E[Y|W=0]\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-average-treatment-effect",
    "href": "slides/04-causal_forests.html#conditional-average-treatment-effect",
    "title": "Causal random forests",
    "section": "Conditional average treatment effect",
    "text": "Conditional average treatment effect\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\]\nwhere \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\hat\\tau_i(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#honesty",
    "href": "slides/04-causal_forests.html#honesty",
    "title": "Causal random forests",
    "section": "Honesty",
    "text": "Honesty\nCausal trees also use honesty.\n\nFor each training observation \\(i\\), \\(Y_i\\) is used for ONE of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\n\nSubsampling: training data for a given tree are not sampled with replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#causal-random-forest",
    "href": "slides/04-causal_forests.html#causal-random-forest",
    "title": "Causal random forests",
    "section": "Causal random forest",
    "text": "Causal random forest\nSuppose we fit a \\(B\\) causal trees to a training set of size \\(n\\).\nWe want to compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nMean of outcomes \\(Y_i\\) that fall into the same terminal leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#honesty-and-subsampling",
    "href": "slides/04-causal_forests.html#honesty-and-subsampling",
    "title": "Causal random forests",
    "section": "Honesty and subsampling",
    "text": "Honesty and subsampling\na tree is honest if, for each training example i, it only uses the response Yi to estimate the within-leaf treatment effect τ using (5) or to decide where to place the splits, but not both."
  },
  {
    "objectID": "slides/04-causal_forests.html#honest-and-subsampling",
    "href": "slides/04-causal_forests.html#honest-and-subsampling",
    "title": "Causal random forests",
    "section": "Honest and subsampling",
    "text": "Honest and subsampling\nCausal trees are grown using “honesty” and “subsampling”\n\nHonesty: For each training observation \\(i\\), \\(Y_i\\) is used for ONE of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\nSubsampling: While Breiman’s random forest used bootstrap resampling with replacement, causal trees use a random subset sampled without replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-mean-predictions",
    "href": "slides/04-causal_forests.html#conditional-mean-predictions",
    "title": "Causal random forests with grf",
    "section": "Conditional mean predictions…",
    "text": "Conditional mean predictions…"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-wrong-take-2",
    "href": "slides/04-causal_forests.html#example-done-wrong-take-2",
    "title": "Causal random forests with grf",
    "section": "Example done wrong, take 2",
    "text": "Example done wrong, take 2\n\nThe model for conditional mean was under-specified.\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nCompute residuals and run Robinson’s regression.\nWhat’d we do wrong?\n\n\n\n\nlibrary(aorsf)\n\nfit_cmean &lt;- orsf(Y ~ X, data = data)\n\nm_x &lt;- predict(fit_cmean, new_data = data)\n\nresid_y &lt;- Y - m_x\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.394643899391376"
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-mean-predictions-1",
    "href": "slides/04-causal_forests.html#conditional-mean-predictions-1",
    "title": "Causal random forests with grf",
    "section": "Conditional mean predictions",
    "text": "Conditional mean predictions"
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-mean-predictions-2",
    "href": "slides/04-causal_forests.html#conditional-mean-predictions-2",
    "title": "Causal random forests",
    "section": "Conditional mean predictions…",
    "text": "Conditional mean predictions…"
  },
  {
    "objectID": "slides/04-causal_forests.html#causal-trees-contd.",
    "href": "slides/04-causal_forests.html#causal-trees-contd.",
    "title": "Causal random forests",
    "section": "Causal trees contd.",
    "text": "Causal trees contd.\nCausal trees are grown using “honesty” and “subsampling” (Wager and Athey 2018).\n\nHonesty: Each training observation is used for one of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\nSubsampling: While Breiman (2001)’s random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forests-define-adaptive-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forests-define-adaptive-neighborhoods",
    "title": "Causal random forests",
    "section": "Random forests define adaptive neighborhoods",
    "text": "Random forests define adaptive neighborhoods\nSuppose we fit a \\(B\\) causal trees to a training set of size \\(n\\).\nWe want to compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nMean of outcomes \\(Y_i\\) that fall into the same terminal leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#adaptive-neighborhoods",
    "href": "slides/04-causal_forests.html#adaptive-neighborhoods",
    "title": "Causal random forests",
    "section": "Adaptive neighborhoods",
    "text": "Adaptive neighborhoods\nSuppose we fit a \\(B\\) regression trees to a training set of size \\(n\\).\nWe want to compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nThe inner sum is the mean of outcomes in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forests-define-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forests-define-neighborhoods",
    "title": "Causal random forests",
    "section": "Random forests define neighborhoods",
    "text": "Random forests define neighborhoods\nSuppose we fit a \\(B\\) regression trees to a training set of size \\(n\\).\nWe want to compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nThe inner sum is the mean of outcomes in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#re-express-the-prediction-sums",
    "href": "slides/04-causal_forests.html#re-express-the-prediction-sums",
    "title": "Causal random forests",
    "section": "Re-express the prediction sums",
    "text": "Re-express the prediction sums\nPull \\(Y_i\\) out of the sum that depends on \\(b\\):\n\\[\\begin{equation*}\n\\begin{split}\np &= \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|} \\\\\n&= \\sum_{i=1}^{n} Y_i \\sum_{b=1}^{B} \\frac{1\\{Xi \\in L_b(x)\\}} {B \\cdot |L_b(x)|} \\\\\n& = \\sum_{i=1}^{n} Y_i \\color{blue}{\\alpha_i(x)},\n\\end{split}\n\\end{equation*}\\]\n\n\\(\\alpha_i(x) \\propto\\) no. of times observation \\(i\\) lands in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#plug-weights-in-to-lm",
    "href": "slides/04-causal_forests.html#plug-weights-in-to-lm",
    "title": "Causal random forests with grf",
    "section": "Plug weights in to lm",
    "text": "Plug weights in to lm\nInstead of defining neighborhood boundaries, weight by similarity:\n\\[\n\\hat\\tau_i(x) := \\text{lm}\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\] This forest-localized version of Robinson’s regression, paired with honesty and subsampling, gives asymptotic guarantees for estimation and inference (Wager and Athey 2018):\n\nPointwise consistency for the true treatment effect.\nAsymptotically Gaussian and centered sampling distribution."
  },
  {
    "objectID": "slides/04-causal_forests.html#summary",
    "href": "slides/04-causal_forests.html#summary",
    "title": "Causal random forests",
    "section": "Summary",
    "text": "Summary\nThe causal forest has three complementary pieces:\n\\[\n\\hat\\tau_i(x) := \\text{lm}\\Biggl( Y_i - \\color{green}{\\hat m^{(-i)}(X_i)} \\sim W_i - \\color{red}{\\hat e^{(-i)}(X_i)}, w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\]\n\n\\(\\color{green}{\\hat m^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[Y|X]\\)\n\\(\\color{red}{\\hat e^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[W|X]\\)\n\\(\\color{blue}{\\alpha_i(x)}\\) are the similarity weights from a forest of causal trees"
  },
  {
    "objectID": "slides/04-causal_forests.html#cate-with-causal-forests",
    "href": "slides/04-causal_forests.html#cate-with-causal-forests",
    "title": "Causal random forests",
    "section": "CATE with causal forests",
    "text": "CATE with causal forests\nYou could compute average treatment effect (ATE) as the mean of CATEs:\n\\[\\hat\\tau = \\frac{1}{n}\\sum_{i=1}^n \\hat\\tau_i(x)\\] But it turns out there are better approaches"
  },
  {
    "objectID": "slides/04-causal_forests.html#three-main-components",
    "href": "slides/04-causal_forests.html#three-main-components",
    "title": "Causal random forests with grf",
    "section": "Three main components",
    "text": "Three main components\nThe procedure to estimate \\(\\hat\\tau_i\\) has three pieces:\n\\[\n\\hat\\tau_i(x) := \\text{lm}\\Biggl( Y_i - \\color{green}{\\hat m^{(-i)}(X_i)} \\sim W_i - \\color{red}{\\hat e^{(-i)}(X_i)}, w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\]\n\n\\(\\color{green}{\\hat m^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[Y|X]\\)\n\\(\\color{red}{\\hat e^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[W|X]\\)\n\\(\\color{blue}{\\alpha_i(x)}\\) are the similarity weights from a causal random forest"
  },
  {
    "objectID": "slides/04-causal_forests.html#summaries-of-cates",
    "href": "slides/04-causal_forests.html#summaries-of-cates",
    "title": "Causal random forests with grf",
    "section": "Summaries of CATEs",
    "text": "Summaries of CATEs\nYou could compute average treatment effect (ATE) as the mean of CATEs:\n\\[\\hat\\tau = \\frac{1}{n}\\sum_{i=1}^n \\hat\\tau_i(x)\\] But the augmented inverse probability weighted ATE is better:\n\\[\n\\hat \\tau_{AIPW} = \\frac{1}{n} \\sum_{i=1}^{n}\\left( \\overbrace{\\tau(X_i)}^{\\text{Initial estimate}} + \\overbrace{\\frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]}}^{\\text{debiasing weight}} \\cdot \\overbrace{\\left(Y_i - \\mu(X_i, W_i)\\right)}^{\\text{residual}} \\right)\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#summaries-of-cates-contd.",
    "href": "slides/04-causal_forests.html#summaries-of-cates-contd.",
    "title": "Causal random forests with grf",
    "section": "Summaries of CATEs contd.",
    "text": "Summaries of CATEs contd.\nFor simplicity, re-write the augmented inverse probability ATE as\n\\[\\hat\\tau = \\frac{1}{n}\\sum_{i=1}^n \\hat\\Gamma_i(x),\\] With a vector of these \\(\\Gamma_i\\)’s, define:\n\nAverage treatment effect (ATE) = mean(gamma)\nBest linear projection (BLP) = lm(gamma ~ X)"
  },
  {
    "objectID": "slides/04-causal_forests.html#load-your-data",
    "href": "slides/04-causal_forests.html#load-your-data",
    "title": "Causal random forests",
    "section": "Load your data",
    "text": "Load your data\nFirst, load global objects and data:\n\n# loads packages and R functions\ntargets::tar_load_globals()\n\n# loads a specific target\ntar_load(data_sim_1)\n\nSecond, coerce data to grf format:\n\n# helper function for grf data prep \ndata_grf &lt;- data_coerce_grf(data_sim_1)\n\n# just a view of the X matrix\ndata_grf$X[1:5, 1:4]\n\n          age biomarker_1 biomarker_2 biomarker_3\n[1,] 66.27077  0.74731375 -0.74236839  -1.7127343\n[2,] 65.51047 -0.56219488 -0.05134715   0.5464561\n[3,] 71.41878  0.01334597 -0.18433741   0.8256376\n[4,] 59.47485  0.79510234 -1.32408422  -1.2779016\n[5,] 58.47673 -0.29311302  0.19057088   2.5377970"
  },
  {
    "objectID": "slides/04-causal_forests.html#fit-your-forest",
    "href": "slides/04-causal_forests.html#fit-your-forest",
    "title": "Causal random forests",
    "section": "Fit your forest",
    "text": "Fit your forest\nThird, fit the causal forest:\n\nfit_grf &lt;- \n  causal_survival_forest(\n    X = data_grf$X, # covariates\n    Y = data_grf$Y, # time to event\n    W = data_grf$W, # treatment status\n    D = data_grf$D, # event status\n    horizon = 3, # 3-year horizon\n    target = 'RMST' # survival time\n  )\n\nfit_grf\n\nGRF forest object of type causal_survival_forest \nNumber of trees: 2000 \nNumber of training samples: 974 \nVariable importance: \n    1     2     3     4     5 \n0.234 0.342 0.206 0.194 0.023"
  },
  {
    "objectID": "slides/04-causal_forests.html#set-up",
    "href": "slides/04-causal_forests.html#set-up",
    "title": "Causal random forests with grf",
    "section": "Set up",
    "text": "Set up\nAssume the survival setting:\n\\[\\begin{equation}\n  Y_i =\n    \\begin{cases}\n      T_i & \\text{if } \\, T_i \\leq C_i \\\\\n      C_i & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nWhere \\(T_i\\) is time to event and (\\(C_i\\)) is time to censoring. Define\n\\[\\begin{equation}\nD_i =\n    \\begin{cases}\n      1 & \\text{if } \\, T_i \\leq C_i \\\\\n      0 & \\text{otherwise.}\n    \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#fitting-the-forest",
    "href": "slides/04-causal_forests.html#fitting-the-forest",
    "title": "Causal random forests with grf",
    "section": "Fitting the forest",
    "text": "Fitting the forest\nThird, fit the causal survival forest:\n\nfit_grf &lt;- causal_survival_forest(\n  X = data_grf$X, # covariates\n  Y = data_grf$Y, # time to event\n  W = data_grf$W, # treatment status\n  D = data_grf$D, # event status\n  horizon = 3, # 3-year horizon\n  # treatment effect will be\n  # measured in terms of the\n  # restricted mean survival time\n  target = 'RMST' \n)\n\nfit_grf\n\nGRF forest object of type causal_survival_forest \nNumber of trees: 2000 \nNumber of training samples: 2438 \nVariable importance: \n    1     2     3     4     5 \n0.145 0.566 0.168 0.107 0.013"
  },
  {
    "objectID": "slides/04-causal_forests.html#validate-what-youve-learned",
    "href": "slides/04-causal_forests.html#validate-what-youve-learned",
    "title": "Causal random forests",
    "section": "Validate what you’ve learned",
    "text": "Validate what you’ve learned\nRemember the \\(\\Gamma_i\\)’s that provide conditional estimates of \\(\\tau\\)? Let’s get them.\n\n# pull the augmented CATEs from the fitted grf object\ngammas &lt;- get_scores(fit_grf)\n\n\nWith gammas, we can compute ATE manually\n\n\nmean(gammas) \n\n[1] -0.4092795\n\n\n\nLast, verify this is what the grf function gives\n\n\naverage_treatment_effect(fit_grf)\n\n   estimate     std.err \n-0.40927954  0.07025854"
  },
  {
    "objectID": "slides/04-causal_forests.html#get-gamma-scores",
    "href": "slides/04-causal_forests.html#get-gamma-scores",
    "title": "Causal random forests with grf",
    "section": "Get \\(\\Gamma\\) scores",
    "text": "Get \\(\\Gamma\\) scores\nRemember the \\(\\Gamma_i\\)’s that provide conditional estimates of \\(\\tau\\)? Let’s get them.\n\n# pull the augmented CATEs from the fitted grf object\ngammas &lt;- get_scores(fit_grf)\n\n\nWith gammas, we can compute ATE manually\n\n\nmean(gammas) \n\n[1] -0.4100232\n\n\n\nVerify this is what the grf function gives\n\n\naverage_treatment_effect(fit_grf)\n\n   estimate     std.err \n-0.41002324  0.04220722"
  },
  {
    "objectID": "slides/04-causal_forests.html#robinsons-residual-on-residual-regression",
    "href": "slides/04-causal_forests.html#robinsons-residual-on-residual-regression",
    "title": "Causal random forests",
    "section": "Robinson’s residual-on-residual regression",
    "text": "Robinson’s residual-on-residual regression\nStart with the partially linear model:\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\] Assume:\n\n\\(E[\\varepsilon_i | X_i, W_i] = 0\\)\nuntreated outcome is given by unknown function \\(f\\),\na treatment assignment shifts the outcome by \\(\\tau\\)."
  },
  {
    "objectID": "slides/04-causal_forests.html#residual-on-residual-regression",
    "href": "slides/04-causal_forests.html#residual-on-residual-regression",
    "title": "Causal random forests",
    "section": "Residual-on-residual regression",
    "text": "Residual-on-residual regression\nStart with the partially linear model:\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\] Assume:\n\n\\(E[\\varepsilon_i | X_i, W_i] = 0\\)\nuntreated outcome is given by unknown function \\(f\\),\na treatment assignment shifts the outcome by \\(\\tau\\)."
  },
  {
    "objectID": "slides/04-causal_forests.html#what-about-censoring",
    "href": "slides/04-causal_forests.html#what-about-censoring",
    "title": "Causal random forests",
    "section": "What about censoring?",
    "text": "What about censoring?\nAssume the survival setting:\n\\[\\begin{equation}\n  Y_i =\n    \\begin{cases}\n      T_i & \\text{if } \\, T_i \\leq C_i \\\\\n      C_i & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nWhere \\(T_i\\) is time to event and (\\(C_i\\)) is time to censoring. Define\n\\[\\begin{equation}\nD_i =\n    \\begin{cases}\n      1 & \\text{if } \\, T_i \\leq C_i \\\\\n      0 & \\text{otherwise.}\n    \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#restricted-mean-survival-time",
    "href": "slides/04-causal_forests.html#restricted-mean-survival-time",
    "title": "Causal random forests",
    "section": "Restricted mean survival time",
    "text": "Restricted mean survival time\nCan’t estimate mean survival time if follow-up ends before all events occur.\n\nWe can estimate a truncated mean: \\(E \\left[ \\text{min}(T, h) \\right]\\)\n\\(h\\) is a truncation parameter (called horizon in the grf package).\nThis is called the restricted mean survival time (RMST)\n\nThe value of \\(D\\) also depends on \\(h\\).\n\nIf \\(T_i = 12\\), \\(h = 10\\), What is \\(D\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#section",
    "href": "slides/04-causal_forests.html#section",
    "title": "Causal random forests",
    "section": "",
    "text": "True event times are obscured by censoring\nEvent times are also obscured by conclusion of follow-up"
  },
  {
    "objectID": "slides/04-causal_forests.html#observed-time-versus-true-time",
    "href": "slides/04-causal_forests.html#observed-time-versus-true-time",
    "title": "Causal random forests with grf",
    "section": "Observed time versus true time",
    "text": "Observed time versus true time\n\nEvent times are obscured by\n\ncensoring\nend of follow-up, i.e., \\(h\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#observed-time-versus-true-time-1",
    "href": "slides/04-causal_forests.html#observed-time-versus-true-time-1",
    "title": "Causal random forests with grf",
    "section": "Observed time versus true time",
    "text": "Observed time versus true time\n\nEvent times are obscured by\n\ncensoring\nend of follow-up, i.e., \\(h\\)\n\nEstimate restricted mean survival time (RMST): \\(E \\left[ \\text{min}(T, h) \\right]\\). See Cui et al. (2023) for more details on adjustment for censoring."
  },
  {
    "objectID": "slides/04-causal_forests.html#the-causal-survival-forest",
    "href": "slides/04-causal_forests.html#the-causal-survival-forest",
    "title": "Causal random forests",
    "section": "The causal survival forest",
    "text": "The causal survival forest\nAssume the survival setting:\n\\[\\begin{equation}\n  Y_i =\n    \\begin{cases}\n      T_i & \\text{if } \\, T_i \\leq C_i \\\\\n      C_i & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nWhere \\(T_i\\) is time to event and (\\(C_i\\)) is time to censoring. Define\n\\[\\begin{equation}\nD_i =\n    \\begin{cases}\n      1 & \\text{if } \\, T_i \\leq C_i \\\\\n      0 & \\text{otherwise.}\n    \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#treatment-effects-for-survival",
    "href": "slides/04-causal_forests.html#treatment-effects-for-survival",
    "title": "Causal random forests with grf",
    "section": "Treatment effects for survival",
    "text": "Treatment effects for survival\nTwo treatment effects can be estimated conditional on \\(h\\).\n\nRMST \\[\\tau(x) = E[\\min(T(1), h) - \\min(T(0), h) \\, | X = x],\\]\nSurvival probability: \\[\\tau(x) = P[T(1) &gt; h \\, | X = x] - P[T(0) &gt; h \\, | X = x].\\] \\(T(1)\\) and \\(T(0)\\) are treated and untreated event times, respectively."
  },
  {
    "objectID": "slides/04-causal_forests.html#the-partially-linear-model",
    "href": "slides/04-causal_forests.html#the-partially-linear-model",
    "title": "Causal random forests with grf",
    "section": "The partially linear model",
    "text": "The partially linear model\nSuppose\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\] Assume:\n\n\\(E[\\varepsilon_i | X_i, W_i] = 0\\)\nuntreated outcome is given by unknown function \\(f\\),\na treatment assignment shifts the outcome by \\(\\tau\\)."
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-grow-causal-trees",
    "href": "slides/04-causal_forests.html#how-to-grow-causal-trees",
    "title": "Causal random forests with grf",
    "section": "How to grow causal trees",
    "text": "How to grow causal trees\nCausal trees are much like standard decision trees, but they maximize\n\\[n_L \\cdot n_R \\cdot (\\hat{\\tau}_L-\\hat{\\tau}_R)^2\\]\nwhere residual-on-residual regression is used to estimate \\(\\hat{\\tau}_L\\) and \\(\\hat{\\tau}_R\\)\n\ngrf estimates \\(\\hat \\tau\\) once in the parent node and uses “influence functions” to approximate how \\(\\hat\\tau\\) would change if an observation moved from one child node to the other (Wager and Athey 2018).\nPredictions from leaves are \\(E[Y|W=1] - E[Y|W=0]\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#how-are-causal-trees-unique",
    "href": "slides/04-causal_forests.html#how-are-causal-trees-unique",
    "title": "Causal random forests",
    "section": "How are causal trees unique?",
    "text": "How are causal trees unique?\nCausal trees use “honesty” and “subsampling” (Wager and Athey 2018).\n\nHonesty: Each training observation is used for one of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\nSubsampling: While Breiman (2001)’s random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#back-to-the-partial-linear-model",
    "href": "slides/04-causal_forests.html#back-to-the-partial-linear-model",
    "title": "Causal random forests with grf",
    "section": "Back to the partial linear model",
    "text": "Back to the partial linear model\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\]\nwhere \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\hat\\tau_i(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-grow-causal-trees-1",
    "href": "slides/04-causal_forests.html#how-to-grow-causal-trees-1",
    "title": "Causal random forests with grf",
    "section": "How to grow causal trees",
    "text": "How to grow causal trees\nCausal trees use “honesty” and “subsampling” (Wager and Athey 2018).\n\nHonesty: Each training observation is used for one of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\nSubsampling: While Breiman (2001)’s random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forests-adaptive-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forests-adaptive-neighborhoods",
    "title": "Causal random forests",
    "section": "Random forests adaptive neighborhoods",
    "text": "Random forests adaptive neighborhoods\nSuppose we fit a \\(B\\) regression trees to a training set of size \\(n\\).\nWe want to compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nThe inner sum is the mean of outcomes in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods-1",
    "href": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods-1",
    "title": "Causal random forests with grf",
    "section": "Random forest adaptive neighborhoods",
    "text": "Random forest adaptive neighborhoods\nPull \\(Y_i\\) out of the sum that depends on \\(b\\):\n\\[\\begin{equation*}\n\\begin{split}\np &= \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|} \\\\\n&= \\sum_{i=1}^{n} Y_i \\sum_{b=1}^{B} \\frac{1\\{Xi \\in L_b(x)\\}} {B \\cdot |L_b(x)|} \\\\\n& = \\sum_{i=1}^{n} Y_i \\color{blue}{\\alpha_i(x)},\n\\end{split}\n\\end{equation*}\\]\n\n\\(\\alpha_i(x) \\propto\\) no. of times observation \\(i\\) lands in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods",
    "title": "Causal random forests with grf",
    "section": "Random forest adaptive neighborhoods",
    "text": "Random forest adaptive neighborhoods\nSuppose we fit a random forest with \\(B\\) trees to a training set of size \\(n\\), and we compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nThe inner sum is the mean of outcomes in the same leaf as \\(x\\)\nThis generalizes to causal random forests (it’s easier to write with regression trees)."
  },
  {
    "objectID": "slides/04-causal_forests.html#set-up-1",
    "href": "slides/04-causal_forests.html#set-up-1",
    "title": "Causal random forests",
    "section": "Set up",
    "text": "Set up\nFirst, we’ll prepare the data:\n\n# loads packages and R functions\ntargets::tar_load_globals()\n\n# loads a specific target\ntar_load(data_sim_1)\n\nSecond, coerce data to grf format:\n\n# helper function for grf data prep \ndata_grf &lt;- data_coerce_grf(data_sim_1)\n\n# just a view of the X matrix\nhead(data_grf$X)\n\n          age biomarker_1 biomarker_2 biomarker_3 sex_female\n[1,] 66.27077  0.74731375 -0.74236839  -1.7127343          1\n[2,] 65.51047 -0.56219488 -0.05134715   0.5464561          1\n[3,] 71.41878  0.01334597 -0.18433741   0.8256376          0\n[4,] 59.47485  0.79510234 -1.32408422  -1.2779016          1\n[5,] 58.47673 -0.29311302  0.19057088   2.5377970          1\n[6,] 67.24764  0.07806861 -1.09980944   1.0296027          0"
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-the-ate",
    "href": "slides/04-causal_forests.html#estimating-the-ate",
    "title": "Causal random forests with grf",
    "section": "Estimating the ATE",
    "text": "Estimating the ATE\nFirst, we’ll prepare the data:\n\n# loads packages and R functions\ntargets::tar_load_globals()\n\n# loads a specific target\ntar_load(data_sim_1)\n\nSecond, coerce data to grf format:\n\n# helper function for grf data prep \ndata_grf &lt;- data_coerce_grf(data_sim_1$values)\n\n# just a view of the X matrix\nhead(data_grf$X, n = 2)\n\n          age biomarker_1  biomarker_2 biomarker_3 sex_female\n[1,] 66.27077   0.5851853  0.007190105  0.08550595          1\n[2,] 65.51047   1.1224445 -0.750371586  0.36835322          1"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn",
    "href": "slides/04-causal_forests.html#your-turn",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nOpen classwork/04-causal_forests.qmd and complete Exercise 1\n\nReminder: Red sticky note for help, green sticky when you finish.\nNote: the data_coerce_grf() function can save you lots of time."
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-the-blp",
    "href": "slides/04-causal_forests.html#estimating-the-blp",
    "title": "Causal random forests with grf",
    "section": "Estimating the BLP",
    "text": "Estimating the BLP\nThe BLP (Semenova and Chernozhukov 2021):\n\nIs estimated by regressing a set of covariates on \\(\\Gamma\\).\nCan be estimated for a subset of covariates\nCan be estimated for a subset of observations.\nSummarizes heterogeneous treatment effects conditional on covariates.\n\nYou can estimate BLP manually:\n\ndata_blp &lt;- bind_cols(gamma = gammas, data_grf$X)\n\nfit_blp &lt;- lm(gamma ~ ., data = data_blp)"
  },
  {
    "objectID": "slides/04-causal_forests.html#two-ways-to-estimate",
    "href": "slides/04-causal_forests.html#two-ways-to-estimate",
    "title": "Causal random forests",
    "section": "Two ways to estimate",
    "text": "Two ways to estimate\nReplicate grf results (good to know what’s happening underneath the hood):\n\n\nlmtest::coeftest(fit_blp, \n                 vcov = sandwich::vcovCL, \n                 type = 'HC3')\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.3469986  1.0274928  0.3377   0.7357\nage         -0.0118992  0.0156814 -0.7588   0.4482\nbiomarker_1 -0.1025880  0.0683212 -1.5016   0.1335\nbiomarker_2 -0.0027167  0.0711906 -0.0382   0.9696\nbiomarker_3 -0.0316154  0.0710252 -0.4451   0.6563\nsex_female   0.0161499  0.1411142  0.1144   0.9089\n\n\n\n\n\nbest_linear_projection(fit_grf, data_grf$X)\n\n\nBest linear projection of the conditional average treatment effect.\nConfidence intervals are cluster- and heteroskedasticity-robust (HC3):\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.3469986  1.0274928  0.3377   0.7357\nage         -0.0118992  0.0156814 -0.7588   0.4482\nbiomarker_1 -0.1025880  0.0683212 -1.5016   0.1335\nbiomarker_2 -0.0027167  0.0711906 -0.0382   0.9696\nbiomarker_3 -0.0316154  0.0710252 -0.4451   0.6563\nsex_female   0.0161499  0.1411142  0.1144   0.9089"
  },
  {
    "objectID": "slides/04-causal_forests.html#what-happens-underneath-the-grf-hood",
    "href": "slides/04-causal_forests.html#what-happens-underneath-the-grf-hood",
    "title": "Causal random forests with grf",
    "section": "What happens underneath the grf hood",
    "text": "What happens underneath the grf hood\nHere’s how you can replicate grf results:\n\n\nlmtest::coeftest(fit_blp, \n                 vcov = sandwich::vcovCL, \n                 type = 'HC3')\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.2472386  0.6076333 -2.0526  0.04022 *\nage          0.0135417  0.0092602  1.4624  0.14377  \nbiomarker_1 -0.0901502  0.0430122 -2.0959  0.03619 *\nbiomarker_2 -0.0318047  0.0454729 -0.6994  0.48436  \nbiomarker_3  0.0410478  0.0409610  1.0021  0.31638  \nsex_female  -0.0831368  0.0844821 -0.9841  0.32518  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nbest_linear_projection(fit_grf, data_grf$X)\n\n\nBest linear projection of the conditional average treatment effect.\nConfidence intervals are cluster- and heteroskedasticity-robust (HC3):\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.2472386  0.6076333 -2.0526  0.04022 *\nage          0.0135417  0.0092602  1.4624  0.14377  \nbiomarker_1 -0.0901502  0.0430122 -2.0959  0.03619 *\nbiomarker_2 -0.0318047  0.0454729 -0.6994  0.48436  \nbiomarker_3  0.0410478  0.0409610  1.0021  0.31638  \nsex_female  -0.0831368  0.0844821 -0.9841  0.32518  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn-1",
    "href": "slides/04-causal_forests.html#your-turn-1",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nComplete Exercise 2"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn-2",
    "href": "slides/04-causal_forests.html#your-turn-2",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nComplete Exercise 3"
  },
  {
    "objectID": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect-rate",
    "href": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect-rate",
    "title": "Causal random forests",
    "section": "Rank-Weighted Average Treatment Effect (RATE)",
    "text": "Rank-Weighted Average Treatment Effect (RATE)\nQuestions:\n\nIs there any heterogeneity present in a conditional treatment effect?\nHow good is a treatment prioritization rule at distinguishing sub-populations with different treatment effects?"
  },
  {
    "objectID": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect",
    "href": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect",
    "title": "Causal random forests with grf",
    "section": "Rank-Weighted Average Treatment Effect",
    "text": "Rank-Weighted Average Treatment Effect\nWhile ATE and BLP are helpful, they do not tell us the following:\n\nHow good is a treatment prioritization rule at distinguishing sub-populations with different conditional treatment effects?\nIs there any heterogeneity present in a conditional treatment effect?\n\nThe rank-weighted average treatment effect (RATE) answers both of these."
  },
  {
    "objectID": "slides/04-causal_forests.html#treatment-prioritization-rules",
    "href": "slides/04-causal_forests.html#treatment-prioritization-rules",
    "title": "Causal random forests with grf",
    "section": "Treatment prioritization rules",
    "text": "Treatment prioritization rules\nSuppose we have a treatment that benefits some (not all) adults. Who should initiate treatment? A treatment prioritization rule can help:\n\nhigh score for those likely to benefit from treatment.\nlow score for those likely to have a small/negative benefit from treatment.\n\nRisk prediction models can be a treatment prioritization rule:\n\nInitiate antihypertensive medication if predicted risk for cardiovascular disease is high."
  },
  {
    "objectID": "slides/04-causal_forests.html#how-rate-works",
    "href": "slides/04-causal_forests.html#how-rate-works",
    "title": "Causal random forests",
    "section": "How RATE works",
    "text": "How RATE works\nThe basic idea:\n\nChop the population up into subgroups based on the prioritization rule, e.g., by decile of score.\nEstimate the ATE in each group, separately, and compare to the overall estimated ATE from treating everyone\nPlot the difference between group-specific ATE and the overall ATE for each of the groups\n\nExample: the Targeting Operator Characteristic (TOC)"
  },
  {
    "objectID": "slides/04-causal_forests.html#targeting-operator-characteristic-toc",
    "href": "slides/04-causal_forests.html#targeting-operator-characteristic-toc",
    "title": "Causal random forests with grf",
    "section": "Targeting Operator Characteristic (TOC)",
    "text": "Targeting Operator Characteristic (TOC)\n\n\nCreate groups by including the top q\\(^\\text{th}\\) fraction of individuals with the largest prioritization score.\nUse many values of \\(q\\) to make the pattern more curve-like\nMotivation: Receiver Operating Characteristic (ROC) curve, a widely used metric for assessing discrimination of predictions."
  },
  {
    "objectID": "slides/04-causal_forests.html#application",
    "href": "slides/04-causal_forests.html#application",
    "title": "Causal random forests",
    "section": "Application",
    "text": "Application\nLet’s use RATE to see how well risk prediction works as a treatment prioritization rule.\n\nlibrary(aorsf)\n\nfit_orsf &lt;- orsf(time + status ~ .,\n                 data = data_sim_1$values,\n                 oobag_pred_horizon = 3)\n\nprd_risk &lt;- as.numeric(fit_orsf$pred_oobag)\n\nprd_rate &lt;- \n  rank_average_treatment_effect(fit_grf, \n                                priorities = prd_risk)"
  },
  {
    "objectID": "slides/04-causal_forests.html#rate-is-the-area-underneath-toc",
    "href": "slides/04-causal_forests.html#rate-is-the-area-underneath-toc",
    "title": "Causal random forests",
    "section": "RATE is the area underneath TOC",
    "text": "RATE is the area underneath TOC"
  },
  {
    "objectID": "slides/04-causal_forests.html#rate-area-underneath-toc",
    "href": "slides/04-causal_forests.html#rate-area-underneath-toc",
    "title": "Causal random forests with grf",
    "section": "RATE: area underneath TOC",
    "text": "RATE: area underneath TOC\n\nRATE is estimated by taking the area underneath the TOC curve.\n\\[\\textrm{RATE} = \\int_0^1 \\textrm{TOC}(q) dq .\\]\nAs \\(\\tau(X_i)\\) approaches a constant, RATE approaches 0."
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-rate",
    "href": "slides/04-causal_forests.html#estimating-rate",
    "title": "Causal random forests",
    "section": "Estimating RATE",
    "text": "Estimating RATE\n(Yadlowsky et al. 2021)"
  },
  {
    "objectID": "slides/04-causal_forests.html#rate-of-prediction-model",
    "href": "slides/04-causal_forests.html#rate-of-prediction-model",
    "title": "Causal random forests with grf",
    "section": "RATE of prediction model",
    "text": "RATE of prediction model\nLet’s use RATE to see how well risk prediction works as a treatment prioritization rule.\n\nlibrary(aorsf)\n\nfit_orsf &lt;- orsf(time + status ~ .,\n                 data = data_sim_1$values,\n                 oobag_pred_horizon = 3)\n\n# important to use oobag!\nprd_risk &lt;- as.numeric(fit_orsf$pred_oobag)\n\nprd_rate &lt;- \n  rank_average_treatment_effect(fit_grf, \n                                priorities = prd_risk)"
  },
  {
    "objectID": "slides/04-causal_forests.html#not-good",
    "href": "slides/04-causal_forests.html#not-good",
    "title": "Causal random forests with grf",
    "section": "Not good",
    "text": "Not good"
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-rate-from-cate",
    "href": "slides/04-causal_forests.html#estimating-rate-from-cate",
    "title": "Causal random forests with grf",
    "section": "Estimating RATE from CATE",
    "text": "Estimating RATE from CATE\nAn intuitive way to assign treatment priority is to use the CATE: \\(\\hat\\tau(X_i)\\)\n\n\\(\\hat\\tau(X_i)\\) should not be estimated and evaluated using the same data\nUse split-sample estimation or cross-fitting (Yadlowsky et al. 2021).\n\nAs a preliminary step, we’ll split our data in to training and testing sets\n\ntrain_index &lt;- sample(x = nrow(data_sim_1$values), size = 1250)\n\ndata_trn &lt;- data_sim_1$values[train_index, ]\ndata_tst &lt;- data_sim_1$values[-train_index, ]\n\ndata_trn_grf &lt;- data_coerce_grf(data_trn)\ndata_tst_grf &lt;- data_coerce_grf(data_tst)"
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-evaluate-treatment-prioritization",
    "href": "slides/04-causal_forests.html#how-to-evaluate-treatment-prioritization",
    "title": "Causal random forests with grf",
    "section": "How to evaluate treatment prioritization",
    "text": "How to evaluate treatment prioritization\nThe basic idea:\n\nChop the population up into subgroups based on the prioritization rule, e.g., by decile of score.\nEstimate the ATE in each group, separately, and compare to the overall estimated ATE from treating everyone\nPlot the difference between group-specific ATE and the overall ATE for each of the groups\n\nExample: the Targeting Operator Characteristic (TOC)"
  },
  {
    "objectID": "slides/04-causal_forests.html#split-sample-approach",
    "href": "slides/04-causal_forests.html#split-sample-approach",
    "title": "Causal random forests",
    "section": "Split sample approach",
    "text": "Split sample approach\nWe fit one forest with training data to estimate CATE and fit another forest with testing data to evaluate the CATE estimates:\n\n\nfit_trn_grf &lt;- \n  causal_survival_forest(\n    X = data_trn_grf$X, Y = data_trn_grf$Y,\n    W = data_trn_grf$W, D = data_trn_grf$D,\n    horizon = 3, target = 'RMST' \n  )\n\nfit_tst_grf &lt;- \n  causal_survival_forest(\n    X = data_tst_grf$X, Y = data_tst_grf$Y,\n    W = data_tst_grf$W, D = data_tst_grf$D,\n    horizon = 3, target = 'RMST' \n  )"
  },
  {
    "objectID": "slides/04-causal_forests.html#split-sample-approach-step-2",
    "href": "slides/04-causal_forests.html#split-sample-approach-step-2",
    "title": "Causal random forests",
    "section": "Split sample approach; step 2",
    "text": "Split sample approach; step 2"
  },
  {
    "objectID": "slides/04-causal_forests.html#split-sample-approach-step-2-1",
    "href": "slides/04-causal_forests.html#split-sample-approach-step-2-1",
    "title": "Causal random forests",
    "section": "Split sample approach; step 2",
    "text": "Split sample approach; step 2"
  },
  {
    "objectID": "slides/04-causal_forests.html#fitting",
    "href": "slides/04-causal_forests.html#fitting",
    "title": "Causal random forests with grf",
    "section": "Fitting",
    "text": "Fitting\nWe fit one forest with training data to estimate CATE and fit another forest with testing data to evaluate the CATE estimates:\n\n\nfit_trn_grf &lt;- causal_survival_forest(\n  X = data_trn_grf$X, Y = data_trn_grf$Y,\n  W = data_trn_grf$W, D = data_trn_grf$D,\n  horizon = 3, target = 'RMST' \n)\n\nfit_tst_grf &lt;- causal_survival_forest(\n  X = data_tst_grf$X, Y = data_tst_grf$Y,\n  W = data_tst_grf$W, D = data_tst_grf$D,\n  horizon = 3, target = 'RMST' \n)"
  },
  {
    "objectID": "slides/04-causal_forests.html#predicting",
    "href": "slides/04-causal_forests.html#predicting",
    "title": "Causal random forests with grf",
    "section": "Predicting",
    "text": "Predicting\nWe use the forest fitted to training data to estimate CATE for the testing data. For illustration, we also estimate naive CATE\n\n\n# the fitted forest hasn't\n# seen the testing data\ntau_hat_split &lt;- fit_trn_grf %&gt;% \n  predict(data_tst_grf$X) %&gt;% \n  getElement(\"predictions\")\n\n# Illustration only (don't do this)\ntau_hat_naive &lt;- fit_trn_grf %&gt;% \n  predict(data_trn_grf$X) %&gt;% \n  getElement(\"predictions\")"
  },
  {
    "objectID": "slides/04-causal_forests.html#evaluating",
    "href": "slides/04-causal_forests.html#evaluating",
    "title": "Causal random forests with grf",
    "section": "Evaluating",
    "text": "Evaluating\nWe use the forest fitted to the testing data to evaluate the CATE estimates for observations in the testing data\n\n\nrate_split &lt;- \n  rank_average_treatment_effect(\n    forest = fit_tst_grf, \n    priorities = tau_hat_split, \n    target = \"AUTOC\"\n  )\n\n# Illustration only (don't do this)\nrate_naive &lt;- \n  rank_average_treatment_effect(\n    forest = fit_trn_grf, \n    priorities = tau_hat_naive, \n    target = \"AUTOC\"\n  )"
  },
  {
    "objectID": "slides/04-causal_forests.html#correct-versus-overly-optimistic",
    "href": "slides/04-causal_forests.html#correct-versus-overly-optimistic",
    "title": "Causal random forests with grf",
    "section": "Correct versus overly optimistic",
    "text": "Correct versus overly optimistic\nThe problem with being overly optimistic is it has very high type 1 error"
  },
  {
    "objectID": "slides/03-prediction_forests.html#axis-based-and-oblique-trees",
    "href": "slides/03-prediction_forests.html#axis-based-and-oblique-trees",
    "title": "Develop and evaluate prediction models",
    "section": "Axis based and oblique trees",
    "text": "Axis based and oblique trees\nAxis based trees use a single predictor to split data.\nOblique trees use a weighted combination of two or more predictors"
  },
  {
    "objectID": "slides/03-prediction_forests.html#oblique-tree-first-split",
    "href": "slides/03-prediction_forests.html#oblique-tree-first-split",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique tree: first split",
    "text": "Oblique tree: first split\n\nFirst, split mostly by bill length"
  },
  {
    "objectID": "slides/03-prediction_forests.html#oblique-tree-second-split",
    "href": "slides/03-prediction_forests.html#oblique-tree-second-split",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique tree: second split",
    "text": "Oblique tree: second split\n\nFirst, split mostly by bill length\n\nSecond, make a triangle for the gentoo."
  },
  {
    "objectID": "slides/03-prediction_forests.html#oblique-random-forests",
    "href": "slides/03-prediction_forests.html#oblique-random-forests",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique random forests",
    "text": "Oblique random forests\n\nAggregating randomized trees gives the oblique random forest"
  },
  {
    "objectID": "slides/03-prediction_forests.html#surprisingly-different",
    "href": "slides/03-prediction_forests.html#surprisingly-different",
    "title": "Oblique random forests with aorsf",
    "section": "Surprisingly different!",
    "text": "Surprisingly different!\n\nDespite very many similarities, axis-based and oblique random forests may give different results."
  },
  {
    "objectID": "slides/03-prediction_forests.html#axis-based-vs-oblique",
    "href": "slides/03-prediction_forests.html#axis-based-vs-oblique",
    "title": "Oblique random forests with aorsf",
    "section": "Axis based vs oblique",
    "text": "Axis based vs oblique\n\nBreiman (2001) found oblique random forests compared more favorably to boosting than axis based ones.1\nMenze et al. (2011) coined the term ‘oblique’ random forest and introduced variable importance metrics for it.\nOn benchmarking 190 classifiers on 121 public datasets, Katuwal, Suganthan, and Zhang (2020) found variations on the oblique random forests were the top 3 classifiers.\n\nLeo Breiman named it “Forest-RC” but it later came to be known as oblique"
  },
  {
    "objectID": "slides/03-prediction_forests.html#aorsf-benchmark",
    "href": "slides/03-prediction_forests.html#aorsf-benchmark",
    "title": "Oblique random forests with aorsf",
    "section": "aorsf Benchmark",
    "text": "aorsf Benchmark\nJaeger et al. (2024) tested how a fast version of the oblique random survival forest (aorsf) compared to the original (obliqueRSF). Here’s what we did:\n\nEvaluated both types of oblique random forests in 35 risk prediction tasks (21 datasets)\nMeasured computation time, C-statistic and index of prediction accuracy (IPA).\nUsed Bayesian linear mixed models to test for differences in expected C-statistic and IPA."
  },
  {
    "objectID": "slides/03-prediction_forests.html#references",
    "href": "slides/03-prediction_forests.html#references",
    "title": "Oblique random forests with aorsf",
    "section": "",
    "text": "And making them fast didn’t break their predictions\n\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nJaeger, Byron C, Sawyer Welden, Kristin Lenoir, Jaime L Speiser, Matthew W Segar, Ambarish Pandey, and Nicholas M Pajewski. 2024. “Accelerated and Interpretable Oblique Random Survival Forests.” Journal of Computational and Graphical Statistics 33 (1): 192–207.\n\n\nKatuwal, Rakesh, Ponnuthurai Nagaratnam Suganthan, and Le Zhang. 2020. “Heterogeneous Oblique Random Forest.” Pattern Recognition 99: 107078.\n\n\nMenze, Bjoern H, B Michael Kelm, Daniel N Splitthoff, Ullrich Koethe, and Fred A Hamprecht. 2011. “On Oblique Random Forests.” In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, Part II 22, 453–69. Springer."
  },
  {
    "objectID": "slides/03-prediction_forests.html#background",
    "href": "slides/03-prediction_forests.html#background",
    "title": "Oblique random forests with aorsf",
    "section": "Background",
    "text": "Background\nAxis based trees use a single predictor to split data.\nOblique trees use a weighted combination of two or more predictors"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn-3",
    "href": "slides/04-causal_forests.html#your-turn-3",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nComplete Exercise 4"
  }
]